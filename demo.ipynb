{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59ecfc6-668f-4390-aea3-8b98e7b30651",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generative AI Conversations Made Simple: Overview of Amazon Bedrock Converse API\n",
    "\n",
    "In this demo we will look at how the [Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) on Amazon Bedrock and the problems it solves when performing inference on all text generation models on AWS. \n",
    "\n",
    "[Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)  is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies through a single API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ffd9e",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "As AI models evolve, developers face significant hurdles in keeping pace with changes and leveraging multiple models effectively. \n",
    "\n",
    "This complexity impacts four key areas:\n",
    "\n",
    "1. **Model Versioning:** Staying current with various models and their APIs requires constant learning and adaptation.\n",
    "\n",
    "2. **Conversation Management:** Implementing features like memory for ongoing dialogues adds another layer of complexity.\n",
    "\n",
    "3. **Multi-Model Integration:** Coordinating conversations across different AI models presents technical challenges.\n",
    "\n",
    "4. **API Integration:** Incorporating external data from APIs to enhance AI responses requires sophisticated orchestration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9254421f",
   "metadata": {},
   "source": [
    "## The Solution:\n",
    "\n",
    "The Converse API was developed to simplify this process for developers. \n",
    "\n",
    "By providing the following:\n",
    "\n",
    "1. **Unified API:** Provides a unified API for all models on AWS, allowing seamless integration as models evolve.\n",
    "2. **Simplified handling of multi-turn conversations:** Built-in conversation management, easily enabling memory features.\n",
    "3. **Effortless multi-model interations:** With a unified API communication between models is simplifed.\n",
    "4. **Streamlined tools integration:** Facilitate smooth integration with external APIs to enrich AI responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e02f9c",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before you begin, ensure all prerequisites are in place. You should have:\n",
    "\n",
    "- An [AWS Account](https://aws.amazon.com/free)\n",
    "- The [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) installed and configured with your credentials\n",
    "- Python 3.7+ installed\n",
    "- Requested [access to the model](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html) you want to use\n",
    "\n",
    "For this demo ensure the following models are enabled: \n",
    "- `anthropic.claude-v2:1`\n",
    "- `anthropic.claude-3-haiku-20240307-v1:0`\n",
    "- `meta.llama3-1-8b-instruct-v1:0`\n",
    "- `meta.llama2-13b-chat-v1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45be5b0-2e84-4c24-b11e-d932f125e8ab",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    ">NOTE: If you are running this on your local system I recommend using a python virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ccfbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install boto3 pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8fc0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create a Bedrock Runtime client (You can specify the region_name)\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e4043",
   "metadata": {},
   "source": [
    "## 2. Invoke Model API vs. Converse API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6896f22c",
   "metadata": {},
   "source": [
    "### 2.1 Invoke Model API Example\n",
    "\n",
    "- [InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-invoke.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke model example using Anthropic's Claude Messages API\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html\n",
    "\n",
    "def invoke_model_example(model_id, prompt):\n",
    "    native_request = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\", \n",
    "        \"max_tokens\": 512,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    body = json.dumps(native_request)\n",
    "    \n",
    "    try:\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "            body=body\n",
    "        )\n",
    "        return json.loads(response[\"body\"].read())\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "# Define the model ID\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Define the prompt for the model.\n",
    "prompt = \"What is Amazon VPC?\"\n",
    "response = invoke_model_example(model_id, prompt)\n",
    "\n",
    "# Print the response\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd9c97",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "1. The Invoke Model API requires formatting the request according to the specific model's native API structure.\n",
    "2. We create a dictionary with the prompt and model parameters, then convert it to JSON.\n",
    "3. We call `invoke_model` with the model ID and the formatted request body.\n",
    "4. The response has to then be parsed from JSON and returned.\n",
    "\n",
    "Note: This method requires knowledge of each model's specific API structure and parameters. So this would only work with the Claude 2.1 model that uses the Text Completions API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18481774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke multiple models\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html\n",
    "\n",
    "def invoke_multiple_models_example(model_id, prompt):\n",
    "    # Different request structures for different models\n",
    "    # Remove and discuss that Claude 2 support native messages API and call it out that text completitions as legacy\n",
    "    if \"anthropic.claude\" in model_id:\n",
    "        native_request = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 300,\n",
    "            \"temperature\": 0.5,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    # llama 2 uses [INST] {prompt} [/INST]\n",
    "    elif \"meta.llama2\" in model_id:\n",
    "        native_request = {\n",
    "            \"prompt\": f\"[INST] {prompt} [/INST]\",\n",
    "            \"max_gen_len\": 300,\n",
    "            \"temperature\": 1,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    # llama 3 uses <|begin_of_text|>\n",
    "    elif \"meta.llama3\" in model_id:\n",
    "        native_request = {\n",
    "            \"prompt\": f\"\"\"\n",
    "                <|begin_of_text|>\n",
    "                <|start_header_id|>user<|end_header_id|>\n",
    "                {prompt}\n",
    "                <|eot_id|>\n",
    "                <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "            \"max_gen_len\": 300,\n",
    "            \"temperature\": 1\n",
    "        }\n",
    "    else:\n",
    "        return \"Unsupported model\"\n",
    "\n",
    "    body = json.dumps(native_request)\n",
    "    \n",
    "    try:\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "            body=body\n",
    "        )\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        \n",
    "        # Different response structures for different models\n",
    "        if \"anthropic.claude\" in model_id:\n",
    "            return response_body[\"content\"][0].get(\"text\", \"No message found\")\n",
    "        elif \"meta.llama\" in model_id:\n",
    "            return response_body.get(\"generation\", \"No generation found\") \n",
    "        else:\n",
    "            return \"Unsupported model response\"\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "models = [\n",
    "    \"anthropic.claude-v2:1\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"meta.llama2-70b-chat-v1\",\n",
    "    \"meta.llama3-1-8b-instruct-v1:0\"\n",
    "]\n",
    "prompt = \"What is Amazon VPC?\"\n",
    "\n",
    "for model in models:\n",
    "    response = invoke_multiple_models_example(model, prompt)\n",
    "    print(f\"model_id: {model}\\n{response}\\n\", \"-\"*100, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2f5d7",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "1. The Invoke Model API requires different request structures for different model providers.\n",
    "2. We need to format the prompt differently for Claude models (\"Human: ... Assistant:\") and Llama models (\"[INST] ... [/INST]\").\n",
    "3. The parameter names differ between models (e.g., \"max_tokens_to_sample\" vs \"max_gen_len\").\n",
    "4. The response structure also varies between models, requiring different parsing logic.\n",
    "5. This approach requires maintaining model-specific code, making it harder to switch between or experiment with different models.\n",
    "\n",
    "Challenges:\n",
    "- Need to know and handle each model's specific API structure and parameters.\n",
    "- Code becomes more complex and harder to maintain as you add support for more models.\n",
    "- Switching between models requires changing multiple parts of the code.\n",
    "- Inconsistent parameter names and response structures across models.\n",
    "\n",
    "These challenges highlight why the Converse API was developed to provide a more unified and consistent interface across different models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabf668",
   "metadata": {},
   "source": [
    "### 2.2 Converse API Example\n",
    "\n",
    "The [converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) was created to solve the following problems when performing inference on all text generation models on AWS:\n",
    "\n",
    "- One streamlined unified API for all models on AWS\n",
    "- Standardized way of handling message conversations\n",
    "- Clear LLM System Message handling\n",
    "- Tools integration\n",
    "- Document upload capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97be5d-5d64-4155-acac-24da11a15990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Single prompt with converse API\n",
    "# Not just for conversations although they are supported\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "message_list = []\n",
    "\n",
    "initial_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        { \"text\": \"How are you today?\" } \n",
    "    ],\n",
    "}\n",
    "\n",
    "message_list.append(initial_message)\n",
    "\n",
    "response = bedrock_client.converse(\n",
    "    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    messages=message_list,\n",
    "    inferenceConfig={\n",
    "        \"maxTokens\": 512,\n",
    "        \"temperature\": 0\n",
    "    },\n",
    ")\n",
    "\n",
    "response_message = response['output']['message']\n",
    "print(json.dumps(response_message, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18ca28-6e16-446b-a819-8a817ddb27f6",
   "metadata": {},
   "source": [
    "### 2.3 Converse API Streaming Example\n",
    "\n",
    "If you call `ConverseStream` to stream the response from a model, the stream is returned in the stream response field. The stream emits the following events in the following order.\n",
    "\n",
    "1. `messageStart` (MessageStartEvent). The start event for a message. Includes the role for the message.\n",
    "2. `contentBlockStart` (ContentBlockStartEvent). A Content block start event. Tool use only.\n",
    "3. `contentBlockDelta` (ContentBlockDeltaEvent). A Content block delta event. Includes the partial text that the model generates or the partial input json for tool use.\n",
    "4. `contentBlockStop` (ContentBlockStopEvent). A Content block stop event.\n",
    "5. `messageStop` (MessageStopEvent). The stop event for the message. Includes the reason why the model stopped generating output.\n",
    "6. `metadata` (ConverseStreamMetadataEvent). Metadata for the request. The metadata includes the token usage in usage (TokenUsage) and metrics for the call in metrics (ConverseStreamMetadataEvent).\n",
    "\n",
    "ConverseStream streams a complete content block as a ContentBlockStartEvent event, one or more ContentBlockDeltaEvent events, and a ContentBlockStopEvent event. Use the contentBlockIndex field as an index to correlate the events that make up a content block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a318c-9c57-4009-a74d-61f8fd66536d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ConverseStream \n",
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "def stream_conversation(model_id, messages, system_prompt=\"You are a helpful AI assistant.\"):\n",
    "    try:\n",
    "        response = bedrock_client.converse_stream(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            system=[{\"text\": system_prompt}],\n",
    "            inferenceConfig={\n",
    "                \"temperature\": 0.7,\n",
    "                \"maxTokens\": 500\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        # This loop processes the stream of events from the API. Each event represents a piece of the response.\n",
    "        for event in response.get('stream'):\n",
    "            if 'messageStart' in event:\n",
    "                print(\"Response started:\", event['messageStart']['role'])\n",
    "            elif 'contentBlockDelta' in event:\n",
    "                chunk = event['contentBlockDelta']['delta'].get('text', '')\n",
    "                full_response += chunk\n",
    "                print(chunk, end='', flush=True)  # Print chunks as they arrive\n",
    "            elif 'messageStop' in event:\n",
    "                print(\"\\n\\nResponse completed. Stop reason:\", event['messageStop']['stopReason'])\n",
    "            elif 'metadata' in event:\n",
    "                print(\"\\nMetadata:\", json.dumps(event['metadata'], indent=2))\n",
    "        \n",
    "        return full_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"What is the Amazon VPC?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Streaming response:\\n\")\n",
    "stream_conversation(model_id, messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27cdb9",
   "metadata": {},
   "source": [
    "## 4. Key Features of Converse API\n",
    "\n",
    "### 4.1 Unified API Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f926f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation turn \n",
    "def converse_api_example(model_id, prompt, system_prompt=\"You are an AI assistant that specializes in AWS.\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "    ]\n",
    "    system_prompts = [{\"text\": system_prompt}]\n",
    "    inference_config = {\n",
    "        \"temperature\": 1,\n",
    "        \"maxTokens\": 256\n",
    "    }\n",
    "    try:\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            system=system_prompts,\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        return response[\"output\"][\"message\"]\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "\n",
    "\n",
    "def unified_api_example(prompt):\n",
    "    # List of models to send prompts\n",
    "    models = [\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        \"meta.llama3-1-8b-instruct-v1:0\"\n",
    "    ]\n",
    "\n",
    "    for model in models:\n",
    "        response = converse_api_example(model, prompt)\n",
    "        print(f\"model_id: {model}\\nprompt: {prompt}\\n{response}\\n\", \"-\"*100, \"\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What is cloud computing?\"\n",
    "unified_api_example(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edf47f",
   "metadata": {},
   "source": [
    "### 4.2 Multi-turn Conversations\n",
    "\n",
    "In this example we will use the Converse API to add memory for model conversations.\n",
    "\n",
    "A conversation is a series of messages between the user and the model. You start a conversation by sending a message as a user (user role) to the model. The model, acting as an assistant (assistant role), then generates a response that it returns in a message. If desired, you can continue the conversation by sending further user role messages to the model. To maintain the conversation context, be sure to include any assistant role messages that you receive from the model in subsequent requests. For example code, see [Converse API examples](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-examples).\n",
    "\n",
    ">Messages must have a `[{\"role\": \"user\", \"content\": [{\"text\": \"prompt\"}]}, {\"role\": \"assistant\", \"content\": [{\"text\": \"response\"}]}]` alternating structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_turn_conversation(model_id):\n",
    "    messages = []\n",
    "    # Include system message\n",
    "    system_prompt = \"You are a helpful AI assistant.\"\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Here is the entire message history: \")\n",
    "            for message in messages:\n",
    "                print(message)\n",
    "            break\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": [{\"text\": user_input}]})\n",
    "\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            system=[{\"text\": system_prompt}],\n",
    "            inferenceConfig={\"temperature\": 0.7, \"maxTokens\": 256}\n",
    "        )\n",
    "\n",
    "        ai_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        messages.append({\"role\": \"assistant\", \"content\": [{\"text\": ai_response}]})\n",
    "\n",
    "        print(f\"AI: {ai_response}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "multi_turn_conversation(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a47f0",
   "metadata": {},
   "source": [
    "### 4.3 Call a tool with the Converse API\n",
    "\n",
    "When you using models they \n",
    "\n",
    "This requires you to route requests to and from the model and to and from the APIs in your code.\n",
    "\n",
    "List date and getVPC info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89432704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "def get_date_time():\n",
    "    # Define the Eastern Time Zone\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "\n",
    "    # Get the current time in UTC and convert it to Eastern Time\n",
    "    now = datetime.datetime.now(pytz.utc).astimezone(eastern)\n",
    "\n",
    "    return {\n",
    "        \"date\": now.strftime(\"%Y-%m-%d\"),\n",
    "        \"time\": now.strftime(\"%H:%M:%S\"),\n",
    "        \"timezone\": \"Eastern Time (ET)\"\n",
    "    }\n",
    "\n",
    "def conversation(model_id, messages):\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        system=[{\"text\": \"You are an AI assistant with access to tools that can provide the current date and time. When a user asks about the current date or time, you should use these tools to provide accurate, up-to-date information. Always strive to give helpful and accurate responses when needed use your available tools.\"}],\n",
    "        messages=messages,\n",
    "        toolConfig={\n",
    "            \"tools\": [{\n",
    "                \"toolSpec\": {\n",
    "                    \"name\": \"get_date_time\",\n",
    "                    \"description\": \"Get the current date and time.\",\n",
    "                    \"inputSchema\": {\n",
    "                        \"json\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"date\": {\"type\": \"string\", \"description\": \"Today's current date.\"},\n",
    "                            \"time\": {\"type\": \"string\", \"description\": \"Today's current time.\"}\n",
    "                        },\n",
    "                            \"required\": []\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }],\n",
    "            \"toolChoice\": {\"auto\": {}}\n",
    "        }\n",
    "    )\n",
    "    return response['output']['message']\n",
    "\n",
    "def process_response(ai_response, messages):\n",
    "    for content in ai_response['content']:\n",
    "        if \"text\" in content:\n",
    "            print(f\"AI: {content['text']}\")\n",
    "        if \"toolUse\" in content:\n",
    "            tool_use = content['toolUse']\n",
    "            print(f\"Checking date and time.....\")\n",
    "            if tool_use['name'] == \"get_date_time\":\n",
    "                date_time = get_date_time()\n",
    "                tool_result = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\n",
    "                        \"toolResult\": {\n",
    "                            \"toolUseId\": tool_use['toolUseId'],\n",
    "                            \"content\": [{\"json\": date_time}],\n",
    "                            \"status\": \"success\"\n",
    "                        }\n",
    "                    }]\n",
    "                }\n",
    "                messages.append(tool_result)\n",
    "                result = conversation(model_id, messages)\n",
    "                messages.append(result)\n",
    "                print(f\"AI: {result}\")\n",
    "                return messages\n",
    "    return messages\n",
    "\n",
    "# Example usage\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "messages = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_input}]\n",
    "    })\n",
    "    \n",
    "    ai_response = conversation(model_id, messages)\n",
    "    messages.append(ai_response)\n",
    "    messages = process_response(ai_response, messages)\n",
    "\n",
    "    print(messages)\n",
    "\n",
    "print(\"Conversation ended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9d9fb",
   "metadata": {},
   "source": [
    "## Now go try it out!\n",
    "\n",
    "**Call to action:**\n",
    "1. [Sign Up for AWS](https://aws.amazon.com/free): If you have not already, create an AWS account. \n",
    "2. Access Bedrock: Request model access to Amazon Bedrock in your AWS Console\n",
    "3. [Check out the Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html): Visit the Amazon Bedrock Developer Guide\n",
    "4. [Try the Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html): Use our sample code to make your first Converse API call\n",
    "5. Share your code on social media with the hashtag #AmazonBedrock\n",
    "\n",
    "\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)\n",
    "- [Use the Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html)\n",
    "- [Invoke model](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-invoke.html)\n",
    "- [Anthropic Claude Messages API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-messages-code-examples)\n",
    "- [A developer's guide to Bedrock's new Converse API](https://community.aws/content/2dtauBCeDa703x7fDS9Q30MJoBA/amazon-bedrock-converse-api-developer-guide)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
